import torch
from transformers import TapasTokenizer, TapasForQuestionAnswering

# ローカルのモデルのパスを指定します。
model_path = "path_to_your_model_directory"  # ここを展開したモデルのディレクトリパスに変更してください。

# テーブルデータを定義します。
data = {
    "actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"],
    "age": ["57", "46", "59"],
    "number of movies": ["87", "53", "69"],
    "date of birth": ["18 December 1963", "11 November 1974", "6 May 1961"]
}
table = [list(data.keys())] + list(zip(*data.values()))

# 質問を定義します。
queries = ["How old is Brad Pitt?", "When was George Clooney born?", "How many movies did Leonardo Di Caprio play in?"]

# モデルとトークナイザをロードします。
tokenizer = TapasTokenizer.from_pretrained("google/tapas-large-finetuned-wtq")
model = TapasForQuestionAnswering.from_pretrained(model_path)  # ローカルのモデルをロード

# テーブルと質問をトークナイズします。
inputs = tokenizer(table=table, queries=queries, padding="max_length", return_tensors="pt")

# モデルで予測を行います。
outputs = model(**inputs)

# 予測されたカラムインデックスを取得します。
predicted_column_index = torch.argmax(outputs.logits, dim=2)  # [batch_size, sequence_length]

# 結果を表示します。
for i, query in enumerate(queries):
    column_id = predicted_column_index[i].item()
    predicted_column = table[0][column_id]
    print(f"Query: {query}")
    print(f"Predicted column: {predicted_column}\n")
